{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# import openai\n",
    "from modules.utils.logger import logger\n",
    "# from langchain.document_loaders.audio import WhisperParser\n",
    "from langchain_community.document_loaders.parsers.audio import FasterWhisperParser\n",
    "import base64\n",
    "import tempfile\n",
    "import json\n",
    "import openai\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_path = Path('') / 'data' / 'sample_rec.ogg'\n",
    "assert audio_file_path.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OpenAI simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Transcriptions in module openai.resources.audio.transcriptions object:\n",
      "\n",
      "class Transcriptions(openai._resource.SyncAPIResource)\n",
      " |  Transcriptions(client: 'OpenAI') -> 'None'\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Transcriptions\n",
      " |      openai._resource.SyncAPIResource\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  create(self, *, file: 'FileTypes', model: 'Union[str, AudioModel]', language: 'str | NotGiven' = NOT_GIVEN, prompt: 'str | NotGiven' = NOT_GIVEN, response_format: 'Union[AudioResponseFormat, NotGiven]' = NOT_GIVEN, temperature: 'float | NotGiven' = NOT_GIVEN, timestamp_granularities: \"List[Literal['word', 'segment']] | NotGiven\" = NOT_GIVEN, extra_headers: 'Headers | None' = None, extra_query: 'Query | None' = None, extra_body: 'Body | None' = None, timeout: 'float | httpx.Timeout | None | NotGiven' = NOT_GIVEN) -> 'Transcription | TranscriptionVerbose | str'\n",
      " |      Transcribes audio into the input language.\n",
      " |      \n",
      " |      Args:\n",
      " |        file:\n",
      " |            The audio file object (not file name) to transcribe, in one of these formats:\n",
      " |            flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n",
      " |      \n",
      " |        model: ID of the model to use. Only `whisper-1` (which is powered by our open source\n",
      " |            Whisper V2 model) is currently available.\n",
      " |      \n",
      " |        language: The language of the input audio. Supplying the input language in\n",
      " |            [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will\n",
      " |            improve accuracy and latency.\n",
      " |      \n",
      " |        prompt: An optional text to guide the model's style or continue a previous audio\n",
      " |            segment. The\n",
      " |            [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting)\n",
      " |            should match the audio language.\n",
      " |      \n",
      " |        response_format: The format of the output, in one of these options: `json`, `text`, `srt`,\n",
      " |            `verbose_json`, or `vtt`.\n",
      " |      \n",
      " |        temperature: The sampling temperature, between 0 and 1. Higher values like 0.8 will make the\n",
      " |            output more random, while lower values like 0.2 will make it more focused and\n",
      " |            deterministic. If set to 0, the model will use\n",
      " |            [log probability](https://en.wikipedia.org/wiki/Log_probability) to\n",
      " |            automatically increase the temperature until certain thresholds are hit.\n",
      " |      \n",
      " |        timestamp_granularities: The timestamp granularities to populate for this transcription.\n",
      " |            `response_format` must be set `verbose_json` to use timestamp granularities.\n",
      " |            Either or both of these options are supported: `word`, or `segment`. Note: There\n",
      " |            is no additional latency for segment timestamps, but generating word timestamps\n",
      " |            incurs additional latency.\n",
      " |      \n",
      " |        extra_headers: Send extra headers\n",
      " |      \n",
      " |        extra_query: Add additional query parameters to the request\n",
      " |      \n",
      " |        extra_body: Add additional JSON properties to the request\n",
      " |      \n",
      " |        timeout: Override the client-level default timeout for this request, in seconds\n",
      " |  \n",
      " |  with_raw_response = <functools.cached_property object>\n",
      " |      This property can be used as a prefix for any HTTP method call to return the\n",
      " |      the raw response object instead of the parsed content.\n",
      " |      \n",
      " |      For more information, see https://www.github.com/openai/openai-python#accessing-raw-response-data-eg-headers\n",
      " |  \n",
      " |  with_streaming_response = <functools.cached_property object>\n",
      " |      An alternative to `.with_raw_response` that doesn't eagerly read the response body.\n",
      " |      \n",
      " |      For more information, see https://www.github.com/openai/openai-python#with_streaming_response\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from openai._resource.SyncAPIResource:\n",
      " |  \n",
      " |  __init__(self, client: 'OpenAI') -> 'None'\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from openai._resource.SyncAPIResource:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(client.audio.transcriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 13:47:31,114 [DEBUG] _config.py: load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-12-01 13:47:31,116 [DEBUG] _config.py: load_verify_locations cafile='/Users/av/.local/share/virtualenvs/cmd_recognition-NjpAcSFC/lib/python3.11/site-packages/certifi/cacert.pem'\n",
      "2024-12-01 13:47:31,144 [DEBUG] _base_client.py: Request options: {'method': 'post', 'url': '/audio/transcriptions', 'headers': {'Content-Type': 'multipart/form-data'}, 'files': [('file', SerializationIterator(index=0, iterator=SerializationIterator(index=0, iterator=<_io.BufferedReader name='data/sample_rec.ogg'>)))], 'json_data': {'model': 'whisper-1', 'language': 'en', 'response_format': 'text'}}\n",
      "2024-12-01 13:47:31,150 [DEBUG] _trace.py: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "2024-12-01 13:47:31,151 [DEBUG] base_events.py: Get address info b'api.openai.com':443, type=<SocketKind.SOCK_STREAM: 1>\n",
      "2024-12-01 13:47:31,152 [DEBUG] base_events.py: Getting address info b'api.openai.com':443, type=<SocketKind.SOCK_STREAM: 1> took 0.828ms: [(<AddressFamily.AF_INET: 2>, <SocketKind.SOCK_STREAM: 1>, 6, '', ('162.159.140.245', 443))]\n",
      "2024-12-01 13:47:31,158 [DEBUG] base_events.py: <asyncio.TransportSocket fd=82, family=2, type=1, proto=6, laddr=('127.0.0.1', 50666), raddr=('127.0.0.1', 10011)> connected to 162.159.140.245:443: (<_SelectorSocketTransport fd=82 read=polling write=<idle, bufsize=0>>, <anyio._backends._asyncio.StreamProtocol object at 0x157508790>)\n",
      "2024-12-01 13:47:31,158 [DEBUG] selector_events.py: <_SelectorSocketTransport fd=82 read=idle write=<idle, bufsize=0>> pauses reading\n",
      "2024-12-01 13:47:31,160 [DEBUG] _trace.py: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x15740fe50>\n",
      "2024-12-01 13:47:31,160 [DEBUG] _trace.py: start_tls.started ssl_context=<ssl.SSLContext object at 0x1574fd0a0> server_hostname='api.openai.com' timeout=5.0\n",
      "2024-12-01 13:47:31,161 [DEBUG] selector_events.py: <_SelectorSocketTransport fd=82 read=polling write=<idle, bufsize=0>> resumes reading\n",
      "2024-12-01 13:47:31,191 [DEBUG] selector_events.py: <_SelectorSocketTransport fd=82 read=idle write=<idle, bufsize=0>> pauses reading\n",
      "2024-12-01 13:47:31,193 [DEBUG] _trace.py: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x152f5ad90>\n",
      "2024-12-01 13:47:31,194 [DEBUG] _trace.py: send_request_headers.started request=<Request [b'POST']>\n",
      "2024-12-01 13:47:31,195 [DEBUG] _trace.py: send_request_headers.complete\n",
      "2024-12-01 13:47:31,195 [DEBUG] _trace.py: send_request_body.started request=<Request [b'POST']>\n",
      "2024-12-01 13:47:31,199 [DEBUG] _trace.py: send_request_body.complete\n",
      "2024-12-01 13:47:31,199 [DEBUG] _trace.py: receive_response_headers.started request=<Request [b'POST']>\n",
      "2024-12-01 13:47:31,200 [DEBUG] selector_events.py: <_SelectorSocketTransport fd=82 read=polling write=<idle, bufsize=0>> resumes reading\n",
      "2024-12-01 13:47:32,243 [DEBUG] selector_events.py: <_SelectorSocketTransport fd=82 read=idle write=<idle, bufsize=0>> pauses reading\n",
      "2024-12-01 13:47:32,244 [DEBUG] _trace.py: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 01 Dec 2024 12:47:36 GMT'), (b'Content-Type', b'text/plain; charset=utf-8'), (b'Content-Length', b'5'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-11hwryp8npt2vmcb5vnmpwmx'), (b'openai-processing-ms', b'647'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-request-id', b'req_88d61114b4709e4bbabb680e96bcbeae'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=xr0V_DhuVtDryTVBoJFs3IX._N.fyWx6oHyVDro9qCE-1733057256-1.0.1.1-RUJ9h_wVZApzBEKG2cB2wJ0Zbr.peWhiEkKKNSnPhRLTqIbwIjPtOvfDKScgSlZ7skSdse5cz0SxIfa6xrrCHA; path=/; expires=Sun, 01-Dec-24 13:17:36 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=S445dy02NypQBnzqvJqb1V.8s5w7ZUqFs57gpDXQ1W4-1733057256398-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8eb32fc628b8b890-AMS'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "2024-12-01 13:47:32,246 [INFO] _client.py: HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n",
      "2024-12-01 13:47:32,247 [DEBUG] _trace.py: receive_response_body.started request=<Request [b'POST']>\n",
      "2024-12-01 13:47:32,248 [DEBUG] _trace.py: receive_response_body.complete\n",
      "2024-12-01 13:47:32,249 [DEBUG] _trace.py: response_closed.started\n",
      "2024-12-01 13:47:32,250 [DEBUG] _trace.py: response_closed.complete\n",
      "2024-12-01 13:47:32,250 [DEBUG] _base_client.py: HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: test\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from openai import OpenAI\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),  # This is the default and can be omitted\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "with open(audio_file_path, \"rb\") as audio_file:\n",
    "\n",
    "    # Call the OpenAI Whisper API\n",
    "    response = await client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",\n",
    "        file=audio_file,\n",
    "        response_format=\"text\",\n",
    "        language='en'  # Options: text, json, or srt\n",
    "    )\n",
    "\n",
    "# Output the transcription\n",
    "print(\"Transcription:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Whisper Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_audio_file(file_path):\n",
    "    with open(file_path, \"rb\") as audio_file:\n",
    "        return base64.b64encode(audio_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_audio = encode_audio_file(file)\n",
    "decoded_audio = base64.b64decode(encoded_audio)\n",
    "with tempfile.NamedTemporaryFile(suffix=\".ogg\", delete=False) as temp_audio:\n",
    "            temp_audio.write(decoded_audio)\n",
    "            temp_audio_path = temp_audio.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = FasterWhisperParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " 'device',\n",
       " 'lazy_parse',\n",
       " 'model_size',\n",
       " 'parse']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmd_recognition-NjpAcSFC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
